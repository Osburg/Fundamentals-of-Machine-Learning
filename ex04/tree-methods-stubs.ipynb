{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, prior, n_min=10):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        self.box = m.copy(), M.copy()\n",
    "        \n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features   = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'\n",
    "                \n",
    "                #Split two nodes\n",
    "                left, right = make_density_split_node(node,N,np.random.choice(valid_features, D_try))\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)# your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        m, M = self.box\n",
    "        leaf = self.find_leaf(x)\n",
    "        p = leaf.response*self.prior\n",
    "        if (np.all(x<=M) and np.all(x>=m)):\n",
    "            return p\n",
    "        else:\n",
    "            return 0\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "    \n",
    "    #Volume of the Parent Node\n",
    "    V = np.product(M-m)\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    \n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "        # ANSWER: It is necessary to remove repeated instances in order for thresholds to not cross data points. Otherwise we could end up with children nodes \n",
    "        # which either have no data (if inequality is inclusive) or that have the same data in both left and right nodes\n",
    "        \n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        # Compute candidate thresholds\n",
    "        tj = (data_unique[1:] + data_unique[:-1])/2.0\n",
    "        \n",
    "        \n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            #Compute number of instances left and right of threshold t\n",
    "            Nl = len(data[data[:,j]<t,j]) \n",
    "            Nr = len(data[data[:,j]>t,j])\n",
    "            \n",
    "            #Compute volume of threshold\n",
    "            Vl = V*(t-m[j])/(M[j]-m[j])\n",
    "            Vr = V - Vl\n",
    "            \n",
    "            #Compute LOO error for both left and right children and add them\n",
    "            loo_error = (Nl/(N*Vl))*(Nl/N - 2.0*(Nl-1)/(N-1)) + (Nr/(N*Vr))*(Nr/N - 2.0*(Nr-1)/(N-1))\n",
    "            \n",
    "            # choose the best threshold that minimi\n",
    "            if loo_error < e_min:\n",
    "                e_min = loo_error\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "                \n",
    "                \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    X = node.data[:,j_min]\n",
    "    M_left = M.copy()\n",
    "    m_right = m.copy()\n",
    "    \n",
    "    M_left[j_min] = t_min\n",
    "    m_right[j_min] = t_min\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[X<t_min,:] # store data in left node -- for subsequent splits\n",
    "    left.box = m, M_left # store bounding box in left node\n",
    "    right.data = node.data[X>t_min,:]\n",
    "    right.box = m_right, M\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    m, M = node.box\n",
    "    n = node.data.shape[0]\n",
    "    v = np.product(M-m)\n",
    "    node.response = n/(v*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                #randomly choose D_try features\n",
    "                feature_indices = np.random.choice(D, D_try, replace=False)\n",
    "                #split the node into two\n",
    "                left, right = make_decision_split_node(node, feature_indices)\n",
    "                #put the two nodes on the stack\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                make_decision_leaf_node(node)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        return leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    #(mainly copied from \"density tree\")\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "    for j in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        tj = (data_unique[1:] + data_unique[:-1])/2.0\n",
    "        \n",
    "        for t in tj:\n",
    "            data_left = node.data[:, j].copy()\n",
    "            labels_left = node.labels[data_left<=t].copy()\n",
    "            data_left = data_left[data_left<=t]\n",
    "            \n",
    "            data_right = node.data[:, j].copy()\n",
    "            labels_right = node.labels[data_right>t].copy()\n",
    "            data_right = data_right[data_right>t]\n",
    "            \n",
    "            #Compute number of instances left and right of threshold t\n",
    "            Nl = len(data_left)\n",
    "            Nr = len(data_right)\n",
    "            \n",
    "            #compute the number of instances of each class left and right\n",
    "            classes = np.unique(node.labels)\n",
    "            Nlk = np.empty(len(classes))\n",
    "            Nrk = np.empty(len(classes))\n",
    "            for i, c in enumerate(classes):\n",
    "                Nlk[i] = len(labels_left[labels_left==c])\n",
    "                Nrk[i] = len(labels_right[labels_right==c])\n",
    "            \n",
    "            #Compute LOO error for both left and right children and add them\n",
    "            gini = Nl*(1-np.sum(Nlk**2)/Nl**2)+Nr*(1-np.sum(Nrk**2)/Nr**2)\n",
    "            \n",
    "            # choose the best threshold that minimizes gini\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    X = node.data[:, j_min]\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[X<=t_min]# data in left node\n",
    "    left.labels = node.labels[X<=t_min] # corresponding labels\n",
    "    right.data = node.data[X>t_min]\n",
    "    right.labels = node.labels[X>t_min]\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    node.response = np.sum(1*(node.labels[:, np.newaxis] == np.arange(10)), axis =0)/node.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return (len(np.unique(node.labels))<=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and prepare the digits data\n",
    "digits = load_digits()\n",
    "data = digits[\"data\"]\n",
    "target = digits[\"target\"]\n",
    "labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11f6c0700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Density tree and Decision tree perform rather well, since the diagonal elements of both matrices are in general larger than off diagonal elements. However, it is clear thar decision tree performs much better than density tree, since density tree fails to correctly classify some digits with much more frequency, as  as evinced by some off diagonal elements. Density tree does not classify 8 and 2 digits with as much accuracy as the others. In contrast, Decision tree predicts all  classes with greater accuracy\n"
     ]
    }
   ],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "prediction = np.zeros(len(target))\n",
    "posterior = np.zeros(len(target))\n",
    "for l in labels:\n",
    "    Density = DensityTree()\n",
    "    data_l = data[target == l]\n",
    "    prior_l = len(data_l)/len(data)\n",
    "    Density.train(data_l,prior_l)\n",
    "    for i in range(data.shape[0]):\n",
    "        x = data[i,:]\n",
    "        p = Density.predict(x)\n",
    "        if p>posterior[i]:\n",
    "            posterior[i] = p\n",
    "            prediction[i] = l\n",
    "    \n",
    "Decision = DecisionTree()\n",
    "Decision.train(data, target, n_min=20)\n",
    "prediction_decision = np.zeros(len(target))\n",
    "for i in range(len(target)):\n",
    "    x = data[i,:]\n",
    "    posterior_d = Decision.predict(x)\n",
    "    prediction_decision[i] = np.argmax(posterior_d)\n",
    "\n",
    "conf_matrix_dec = np.zeros((len(labels),len(labels)))\n",
    "conf_matrix_dense = np.zeros((len(labels),len(labels)))\n",
    "\n",
    "for i in range(10):\n",
    "    cond = target == i\n",
    "    conf_matrix_dense[:,i] = np.sum(1*(prediction[cond] == labels[:,np.newaxis]), axis = 1)\n",
    "    conf_matrix_dec[:,i] = np.sum(1*(prediction_decision[cond] == labels[:,np.newaxis]), axis = 1)\n",
    "    \n",
    "plt.figure()\n",
    "print('Density Confusion Matrix')\n",
    "plt.imshow(conf_matrix_dense)\n",
    "plt.show()\n",
    "\n",
    "print('Decision Confusion Matrix')\n",
    "plt.figure()\n",
    "display(plt.imshow(conf_matrix_dec))\n",
    "plt.show()\n",
    "\n",
    "print('Both Density tree and Decision tree perform rather well, since the diagonal elements of both matrices are in general larger than off diagonal elements.\\\n",
    " However, it is clear thar decision tree performs much better than density tree, since density tree fails to correctly classify some digits with much more frequency, as \\\n",
    " as evinced by some off diagonal elements. Density tree does not classify 8 and 2 digits with as much accuracy as the others. In contrast, Decision tree predicts all \\\n",
    " classes with greater accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            bootstrap_indices = np.random.choice(data.shape[0], data.shape[0])\n",
    "            bootstrap_data = data[bootstrap_indices,:]\n",
    "            tree.train(bootstrap_data, prior,n_min)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        #compute average predicted density over all trees\n",
    "        p = 0\n",
    "        for tree in self.trees:\n",
    "            p = p + tree.predict(x)\n",
    "        return p / len(self.trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            bootstrap_indices = np.random.choice(len(labels), len(labels))\n",
    "            bootstrap_data = data[bootstrap_indices,:]\n",
    "            bootstrap_labels = target[bootstrap_indices]\n",
    "            tree.train(bootstrap_data,bootstrap_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        n_t = len(self.trees)\n",
    "        predictions = np.zeros(10)\n",
    "        for tree in self.trees:\n",
    "            p = tree.predict(x)\n",
    "            predictions += p/n_t\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Forest Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density Forest Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The averaging process from the forest greatly improves the accuracy of the classifiers. Decision Forest has an almost perfect classification,even better than the already outstanding single Decision Tree. Additionally, Density forest has a much less rate of failure than a single density tree. \n"
     ]
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "n_trees = 20\n",
    "DecF = DecisionForest(n_trees)\n",
    "DecF.train(data, target)\n",
    "prediction_decision = np.zeros(len(target))\n",
    "prediction_density = np.zeros(len(target))\n",
    "posterior_density = np.zeros(len(target))\n",
    "for i in range(data.shape[0]):\n",
    "    x = data[i,:]\n",
    "    prediction_decision[i] = np.argmax(DecF.predict(x))\n",
    "\n",
    "for l in range(10):\n",
    "    DensityF = DensityForest(n_trees)\n",
    "    data_l = data[target == l] \n",
    "    prior = len(data_l)/len(target)\n",
    "    DensityF.train(data_l,prior)\n",
    "    for i in range(data.shape[0]):\n",
    "        x = data[i,:]\n",
    "        posterior = DensityF.predict(x)\n",
    "        if posterior > posterior_density[i]:\n",
    "            posterior_density[i] = posterior\n",
    "            prediction_density[i] = l\n",
    "\n",
    "\n",
    "conf_matrix_dec = np.zeros((len(labels),len(labels)))\n",
    "conf_matrix_dense = np.zeros((len(labels),len(labels)))\n",
    "for i in range(10):\n",
    "    cond = target == i\n",
    "    conf_matrix_dec[:,i] = np.sum(1*(prediction_decision[cond] == labels[:,np.newaxis]), axis = 1)\n",
    "    conf_matrix_dense[:,i] = np.sum(1*(prediction_density[cond] == labels[:,np.newaxis]), axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "print('Decision Forest Confusion Matrix')\n",
    "plt.imshow(conf_matrix_dec)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "print('Density Forest Confusion Matrix')\n",
    "plt.imshow(conf_matrix_dense)\n",
    "plt.show()\n",
    "\n",
    "print('The averaging process from the forest greatly improves the accuracy of the classifiers. Decision Forest has an almost perfect classification,\\\n",
    "even better than the already outstanding single Decision Tree. Additionally, Density forest has a much less rate of failure than a single density tree. ')\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
