{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4a\n",
    "## 2 Red Cards Study\n",
    "### 2.1 Loading and Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import lsqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['playerShort', 'player', 'club', 'leagueCountry', 'birthday', 'height',\n",
      "       'weight', 'position', 'games', 'victories', 'ties', 'defeats', 'goals',\n",
      "       'yellowCards', 'yellowReds', 'redCards', 'photoID', 'rater1', 'rater2',\n",
      "       'refNum', 'refCountry', 'Alpha_3', 'meanIAT', 'nIAT', 'seIAT',\n",
      "       'meanExp', 'nExp', 'seExp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv(\"CrowdstormingDataJuly1st.csv\", sep=\",\", header=0)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort out (irrelevant) features:\n",
    "- player (playerShort uniquely identifies the player, so player is not needed)\n",
    "- playerShort (the players' names are irrelevant for us, actually...)\n",
    "- photoID (is only needed if we want to classify the skin color by ourselves)\n",
    "- refCountry (we will assume that not the name of the referee country, but the values meanIAT and meanExp are the relevant features regarding the referee country)\n",
    "- nIAT, seIAT (we will just assume the IAT values are a good estimation of the actual value, so we are not interested in the sample size used to determine the IAT)\n",
    "- nExp, seExp (see above)\n",
    "- yellowCards (our examination is only about red cards, not about yellow cards)\n",
    "- club (this is a categorial feature with over 100 categories. The one-hot encoding would therefore create a large amount of new features which drastically increases the dimensionality of the problem. This extra effort is disproportionate to the importance of this feature for this question (maybe(!) some teams play more aggresive than others).)\n",
    "- birthday (We decided against this feature because the data set does not contain information about the date of every single game. This makes it impossible to find out if players get more red cards when they are at a certain age, because the data from this dataset refer to the complete career of the players.)\n",
    "- Alpha_3 (we neglect the nationality of the referee)\n",
    "- games (This is a redundant feature because the total number of games is given by the sum of victories, ties and defeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['leagueCountry', 'height', 'weight', 'position', 'victories', 'ties',\n",
      "       'defeats', 'goals', 'yellowReds', 'redCards', 'rater1', 'rater2',\n",
      "       'refNum', 'meanIAT', 'meanExp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(labels=[\"player\", \"playerShort\", \"photoID\", \"refCountry\", \"nIAT\", \"seIAT\", \"nExp\", \"seExp\", \"yellowCards\", \"club\", \"birthday\", \"Alpha_3\", \"games\"], axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create new features out of existing ones or manipulate them:\n",
    "- rating (we take the mean of rater1 and rater2 as our own rating)\n",
    "- totalReds (we sum up the red and yellow-red cards to the total number of red cards and divide by the number of games, This is our response Y.)\n",
    "- leagueCountry (replace the name of the country by one-hot encoding)\n",
    "- refCount (counts the number of dyads for each referee. Relevant for later drops (see https://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb))\n",
    "- We summarize some categories in position (Goalkeeper, Back, Midfielder, Forward (don't know anything about football, hopefully this makes sense.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take mean of the two skin color ratings\n",
    "df[\"rating\"] = (df[\"rater1\"] + df[\"rater2\"])/2\n",
    "df = df.drop(labels=[\"rater1\", \"rater2\"], axis=1)\n",
    "\n",
    "#sum up red and yellow-red cards\n",
    "df[\"percentageReds\"] = (df[\"redCards\"] + df[\"yellowReds\"])/(df[\"victories\"]+df[\"ties\"]+df[\"defeats\"])\n",
    "df = df.drop(labels=[\"redCards\", \"yellowReds\"], axis=1)\n",
    "\n",
    "#onehot encoding for leagueCountry\n",
    "onehot = pd.get_dummies(df.leagueCountry, prefix=\"Country\")\n",
    "df = df.drop(labels=[\"leagueCountry\"], axis=1)\n",
    "df = pd.concat([df,onehot], axis=1, sort=False)\n",
    "\n",
    "#summarize positions and onehot encoding for positions\n",
    "dic = {\"Right Fullback\":\"Back\",\n",
    "       \"Left Fullback\":\"Back\",\n",
    "       \"Center Back\":\"Back\",\n",
    "       \"Left Midfielder\":\"Midfielder\", \n",
    "       \"Right Midfielder\":\"Midfielder\", \n",
    "       \"Center Midfielder\":\"Midfielder\", \n",
    "       \"Defensive Midfielder\":\"Midfielder\",\n",
    "       \"Attacking Midfielder\":\"Midfielder\",\n",
    "       \"Left Winger\":\"Forward\",\n",
    "       \"Right Winger\":\"Forward\",\n",
    "       \"Center Forward\":\"Forward\"}\n",
    "df = df.replace({\"position\":dic})\n",
    "onehot = pd.get_dummies(df.position, prefix=\"Position\")\n",
    "df = df.drop(labels=[\"position\"], axis=1)\n",
    "df = pd.concat([df,onehot], axis=1, sort=False)\n",
    "\n",
    "\n",
    "#add a column which tracks how many games each ref is involved in\n",
    "#taken from https://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb\n",
    "df['refCount']=0\n",
    "refs=pd.unique(df['refNum'].values.ravel()) #list all unique ref IDs\n",
    "#for each ref, count their dyads\n",
    "for r in refs:\n",
    "    df.loc[df['refNum']==r,\"refCount\"]=len(df[df['refNum']==r])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go on with preparing the data set:\n",
    "- remove rows that contain a NaN-value\n",
    "- remove rows where refCount<22 (for explanation see https://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb. After this we can remove the features \"refNum\" and \"refCount\" because these were only kept for this step.)\n",
    "- normalize the features ties, victories and defeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with NaN in \"rating\" \n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "#remove rows where the \"refCount\"<22\n",
    "df = df.loc[df[\"refCount\"]>21].reset_index()\n",
    "df = df.drop([\"refNum\", \"refCount\", \"index\"], axis=1)\n",
    "\n",
    "#normalize ties, victories and defeats\n",
    "defeats = df[\"defeats\"]/(df[\"defeats\"]+df[\"ties\"]+df[\"victories\"])\n",
    "ties = df[\"ties\"]/(df[\"defeats\"]+df[\"ties\"]+df[\"victories\"])\n",
    "victories = df[\"victories\"]/(df[\"defeats\"]+df[\"ties\"]+df[\"victories\"])\n",
    "df[\"defeats\"] = defeats\n",
    "df[\"ties\"] = ties\n",
    "df[\"victories\"] = victories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tasks we want to apply the LSQR-algorithm. In the lecture we always assumed centralized features and responses. So our last step is to centralize our data. The responses are given by the values in the column \"totalReds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.apply(np.mean, axis=0)\n",
    "df = df - df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>victories</th>\n",
       "      <th>ties</th>\n",
       "      <th>defeats</th>\n",
       "      <th>goals</th>\n",
       "      <th>meanIAT</th>\n",
       "      <th>meanExp</th>\n",
       "      <th>rating</th>\n",
       "      <th>percentageReds</th>\n",
       "      <th>Country_England</th>\n",
       "      <th>Country_France</th>\n",
       "      <th>Country_Germany</th>\n",
       "      <th>Country_Spain</th>\n",
       "      <th>Position_Back</th>\n",
       "      <th>Position_Forward</th>\n",
       "      <th>Position_Goalkeeper</th>\n",
       "      <th>Position_Midfielder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.096034</td>\n",
       "      <td>-5.314496</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>0.691785</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>-0.157667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>0.693521</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.903966</td>\n",
       "      <td>3.685504</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>-0.157667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>0.693521</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.096034</td>\n",
       "      <td>-8.314496</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>0.691785</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>-0.306479</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>0.67205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.903966</td>\n",
       "      <td>3.685504</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>0.767373</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>-0.032667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>-0.306479</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>0.912532</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.096034</td>\n",
       "      <td>-6.314496</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>-0.282667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>-0.286749</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>0.696032</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>0.693521</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113122</th>\n",
       "      <td>5.903966</td>\n",
       "      <td>3.685504</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>0.767373</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.217333</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>-0.306479</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>0.67205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113123</th>\n",
       "      <td>-4.096034</td>\n",
       "      <td>-9.314496</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>0.634959</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>-0.282667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>-0.306479</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>0.67205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113124</th>\n",
       "      <td>7.903966</td>\n",
       "      <td>10.685504</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>0.691785</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>0.713251</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>-0.303968</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>0.693521</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113125</th>\n",
       "      <td>-13.096034</td>\n",
       "      <td>-4.314496</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>-0.308215</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>-0.032667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>-0.286749</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>0.696032</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>-0.306479</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>0.67205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113126</th>\n",
       "      <td>-8.096034</td>\n",
       "      <td>-4.314496</td>\n",
       "      <td>-0.459157</td>\n",
       "      <td>-0.232627</td>\n",
       "      <td>0.691785</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>-0.157667</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>-0.286749</td>\n",
       "      <td>-0.154393</td>\n",
       "      <td>0.696032</td>\n",
       "      <td>-0.254891</td>\n",
       "      <td>0.693521</td>\n",
       "      <td>-0.2114</td>\n",
       "      <td>-0.087468</td>\n",
       "      <td>-0.32795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113127 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           height     weight  victories      ties   defeats     goals  \\\n",
       "0       -0.096034  -5.314496  -0.459157 -0.232627  0.691785 -0.365041   \n",
       "1        4.903966   3.685504   0.540843 -0.232627 -0.308215 -0.365041   \n",
       "2       -2.096034  -8.314496  -0.459157 -0.232627  0.691785 -0.365041   \n",
       "3       10.903966   3.685504  -0.459157  0.767373 -0.308215 -0.365041   \n",
       "4       -2.096034  -6.314496   0.540843 -0.232627 -0.308215 -0.365041   \n",
       "...           ...        ...        ...       ...       ...       ...   \n",
       "113122   5.903966   3.685504  -0.459157  0.767373 -0.308215 -0.365041   \n",
       "113123  -4.096034  -9.314496   0.540843 -0.232627 -0.308215  0.634959   \n",
       "113124   7.903966  10.685504  -0.459157 -0.232627  0.691785 -0.365041   \n",
       "113125 -13.096034  -4.314496   0.540843 -0.232627 -0.308215 -0.365041   \n",
       "113126  -8.096034  -4.314496  -0.459157 -0.232627  0.691785 -0.365041   \n",
       "\n",
       "         meanIAT   meanExp    rating  percentageReds  Country_England  \\\n",
       "0      -0.023132  0.075924 -0.157667       -0.008371         0.713251   \n",
       "1      -0.023132  0.075924 -0.157667       -0.008371         0.713251   \n",
       "2      -0.023132  0.075924  0.717333       -0.008371         0.713251   \n",
       "3      -0.023132  0.075924 -0.032667       -0.008371         0.713251   \n",
       "4      -0.023132  0.075924 -0.282667       -0.008371        -0.286749   \n",
       "...          ...       ...       ...             ...              ...   \n",
       "113122  0.027810  0.035812  0.217333       -0.008371         0.713251   \n",
       "113123  0.027810  0.035812 -0.282667       -0.008371         0.713251   \n",
       "113124  0.027810  0.035812  0.092333       -0.008371         0.713251   \n",
       "113125  0.027810  0.035812 -0.032667       -0.008371        -0.286749   \n",
       "113126  0.027810  0.035812 -0.157667       -0.008371        -0.286749   \n",
       "\n",
       "        Country_France  Country_Germany  Country_Spain  Position_Back  \\\n",
       "0            -0.154393        -0.303968      -0.254891       0.693521   \n",
       "1            -0.154393        -0.303968      -0.254891       0.693521   \n",
       "2            -0.154393        -0.303968      -0.254891      -0.306479   \n",
       "3            -0.154393        -0.303968      -0.254891      -0.306479   \n",
       "4            -0.154393         0.696032      -0.254891       0.693521   \n",
       "...                ...              ...            ...            ...   \n",
       "113122       -0.154393        -0.303968      -0.254891      -0.306479   \n",
       "113123       -0.154393        -0.303968      -0.254891      -0.306479   \n",
       "113124       -0.154393        -0.303968      -0.254891       0.693521   \n",
       "113125       -0.154393         0.696032      -0.254891      -0.306479   \n",
       "113126       -0.154393         0.696032      -0.254891       0.693521   \n",
       "\n",
       "        Position_Forward  Position_Goalkeeper  Position_Midfielder  \n",
       "0                -0.2114            -0.087468             -0.32795  \n",
       "1                -0.2114            -0.087468             -0.32795  \n",
       "2                -0.2114            -0.087468              0.67205  \n",
       "3                -0.2114             0.912532             -0.32795  \n",
       "4                -0.2114            -0.087468             -0.32795  \n",
       "...                  ...                  ...                  ...  \n",
       "113122           -0.2114            -0.087468              0.67205  \n",
       "113123           -0.2114            -0.087468              0.67205  \n",
       "113124           -0.2114            -0.087468             -0.32795  \n",
       "113125           -0.2114            -0.087468              0.67205  \n",
       "113126           -0.2114            -0.087468             -0.32795  \n",
       "\n",
       "[113127 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008733024863475151"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solve the problem using the lsqr algorithm (linear regression)\n",
    "#extract features and responses from the DataFrame\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df.drop(labels=[\"percentageReds\"], axis=1).to_numpy()\n",
    "\n",
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "\n",
    "        #use lsqr algorithm\n",
    "    def train(self, features, labels):\n",
    "        self.beta = lsqr(features,labels)[0]\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_mean = df_mean.drop(labels=[\"percentageReds\"])\n",
    "        y_mean = df_mean[\"percentageReds\"]\n",
    "        return y_mean + np.sum(self.beta*(x-x_mean)) \n",
    "\n",
    "#Test basic functionality\n",
    "regression = LinearRegression()\n",
    "regression.train(X,Y)\n",
    "regression.predict([180, 77, 1.4, 0.8, 1, 0.4, 0.35, 0.5, 1, 0.3, 0.15, 0.3, 0.25, 0.3, 0.21, 0.1, 0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve the problem using regression forestsclass DecisionTree(Tree):\n",
    "# base classes\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    \n",
    "    def find_leaf(self, x):\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node\n",
    "    \n",
    "class RegressionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(RegressionTree, self).__init__()\n",
    "        \n",
    "    def train(self, data, labels, n_min=500):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = np.max([int(np.sqrt(D))-2, 0]) # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        \n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0] # number of instances in present node\n",
    "            if (n >= n_min):\n",
    "                #randomly choose D_try-2 features\n",
    "                feature_indices = np.random.choice(D, D_try, replace=False)\n",
    "                feature_indices = np.append(feature_indices, [0,1,8])\n",
    "                #split the node into two\n",
    "                left, right = make_regression_split_node(node, feature_indices)\n",
    "                #put the two nodes on the stack\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                make_regression_leaf_node(node)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        return leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    #(mainly copied from \"density tree\")\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "\n",
    "    for j in feature_indices:\n",
    "        data_unique = np.sort(np.unique(node.data[:, j]))\n",
    "        tj = (data_unique[1:] + data_unique[:-1])/2.0\n",
    "        \n",
    "        for t in tj:\n",
    "            data_left = node.data[:, j].copy()\n",
    "            labels_left = node.labels[data_left<=t].copy()\n",
    "            data_left = data_left[data_left<=t]\n",
    "            \n",
    "            data_right = node.data[:, j].copy()\n",
    "            labels_right = node.labels[data_right>t].copy()\n",
    "            data_right = data_right[data_right>t]\n",
    "            \n",
    "            #compute mean label value on the left and right\n",
    "            mean_left = np.mean(labels_left)\n",
    "            mean_right = np.mean(labels_right)\n",
    "            \n",
    "            #compute sum of squared deviation from mean label\n",
    "            measure_left = np.sum((labels_left - mean_left)**2)\n",
    "            measure_right = np.sum((labels_right - mean_right)**2)       \n",
    "            \n",
    "            #Compute decision rule\n",
    "            measure = measure_left + measure_right\n",
    "            \n",
    "            # choose the best threshold that minimizes gini\n",
    "            if measure < e_min:\n",
    "                e_min = measure\n",
    "                j_min = j\n",
    "                t_min = t\n",
    "    \n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "    \n",
    "    \n",
    "    X = node.data[:, j_min]\n",
    "    \n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[X<=t_min]# data in left node\n",
    "    left.labels = node.labels[X<=t_min] # corresponding labels\n",
    "    right.data = node.data[X>t_min]\n",
    "    right.labels = node.labels[X>t_min]\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.response = np.mean(node.labels) + df_mean[\"percentageReds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree() for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, data, labels, n_min=1000):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            bootstrap_indices = np.random.choice(len(labels), len(labels))\n",
    "            bootstrap_data = np.array([data[i] for i in bootstrap_indices])\n",
    "            bootstrap_labels = np.array([labels[i] for i in bootstrap_indices])\n",
    "            tree.train(bootstrap_data, bootstrap_labels, n_min=n_min)\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = np.array([])\n",
    "        for tree in self.trees:\n",
    "            predictions = np.append(predictions, tree.predict(x))\n",
    "        return np.mean(predictions)\n",
    "    \n",
    "    def merge(self, forest):\n",
    "        self.trees = self.trees + forest.trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test of basic functionality\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df.drop(labels=[\"percentageReds\"], axis=1).to_numpy()\n",
    "\n",
    "forest = RegressionForest(n_trees=5)\n",
    "forest.train(X, Y, n_min=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n",
      "\n",
      "error rate, linear regression:\n",
      "0.005192709509021647\n"
     ]
    }
   ],
   "source": [
    "#determine the error via cross validation\n",
    "\n",
    "#define function that determines the sum squared error\n",
    "def compute_error(model, test_features, test_labels):\n",
    "    mean_squared_error = 0\n",
    "    n = len(test_features)\n",
    "    \n",
    "    for i in range(n):\n",
    "        mean_squared_error = mean_squared_error + (test_labels[i] - model.predict(test_features[i]))**2\n",
    "    \n",
    "    return mean_squared_error/n\n",
    "\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df.drop(labels=[\"percentageReds\"], axis=1).to_numpy()\n",
    "\n",
    "#number of folds\n",
    "L = 10\n",
    "\n",
    "#create  L folds\n",
    "N = len(X)\n",
    "indices = np.random.choice(N, N, replace=False)\n",
    "X_folds = np.array(np.array_split(X[indices], L), dtype=object) \n",
    "Y_folds = np.array(np.array_split(Y[indices], L), dtype=object)\n",
    "\n",
    "#1. Linear Regression\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    regression = LinearRegression()\n",
    "    regression.train(X_train,Y_train)\n",
    "    error.append(compute_error(regression, X_test, Y_test))\n",
    "error = np.mean(error)\n",
    "\n",
    "#print error\n",
    "print(\"\\nerror rate, linear regression:\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n",
      "\n",
      "error rate, regression forest:\n",
      "0.005265575175396048\n"
     ]
    }
   ],
   "source": [
    "#2. Regression Forest\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    forest = RegressionForest(n_trees=5)\n",
    "    forest.train(X_train,Y_train, n_min=500)\n",
    "    error.append(compute_error(forest, X_test, Y_test))\n",
    "error = np.mean(error)\n",
    "\n",
    "#print error\n",
    "print(\"\\nerror rate, regression forest:\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Answering the Research Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function that shuffles the data in one column\n",
    "def shuffle_data(features, feature_index):\n",
    "    '''\n",
    "    Shuffles the data in the column denoted by feature_index. All other data remain unchanged \n",
    "\n",
    "    features: 2D array, each row stands for one instance, each column for one feature\n",
    "    feature_index: the entries in the feature_index-th column will be shuffled randomly\n",
    "    '''\n",
    "\n",
    "    features = features.transpose()\n",
    "    shuffled_feature = np.random.permutation(features[feature_index])\n",
    "    features[feature_index] = shuffled_feature\n",
    "\n",
    "    return features.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n",
      "\n",
      "error rate, linear regression:\n",
      "0.005183430036888673\n",
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n",
      "\n",
      "error rate, regression tree:\n",
      "0.005268014302182951\n"
     ]
    }
   ],
   "source": [
    "color_rating_index = 8 #index of the color rating in df\n",
    "L = 10 #number of folds\n",
    "\n",
    "#load csv-file where we save the mean squared errors\n",
    "err_data = pd.read_csv(\"errors.txt\", sep=\",\", index_col=False) \n",
    "\n",
    "# load original data set\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df.drop(labels=[\"percentageReds\"], axis=1).to_numpy()\n",
    "\n",
    "#1. Linear Regression\n",
    "#shuffle data\n",
    "Y_shuffled = Y\n",
    "X_shuffled = shuffle_data(X, 8)\n",
    "\n",
    "#create  L folds\n",
    "N = len(X_shuffled)\n",
    "indices = np.random.choice(N, N, replace=False)\n",
    "X_folds = np.array(np.array_split(X_shuffled[indices], L), dtype=object) \n",
    "Y_folds = np.array(np.array_split(Y_shuffled[indices], L), dtype=object)\n",
    "\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    regression = LinearRegression()\n",
    "    regression.train(X_train,Y_train)\n",
    "    error.append(compute_error(regression, X_test, Y_test))\n",
    "error_lr = np.mean(error)\n",
    "\n",
    "#print error and save the value\n",
    "print(\"\\nerror rate, linear regression:\")\n",
    "print(error_lr)\n",
    "\n",
    "#2. Regression Tree\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    forest = RegressionForest(n_trees=5)\n",
    "    forest.train(X_train,Y_train, n_min=500)\n",
    "    error.append(compute_error(forest, X_test, Y_test))\n",
    "error = np.mean(error)\n",
    "\n",
    "#print error and save the value\n",
    "print(\"\\nerror rate, regression tree:\")\n",
    "print(error)\n",
    "err_data.loc[len(err_data)] = [error_lr, error]\n",
    "\n",
    "err_data.to_csv(\"errors.txt\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the following results we run the code from above several times. The first row stands for the results from the unshuffled dataset, the other rows from the shuffled datasets. One can see that the error of some of the rows corresponding to a dataset with shuffled color rating are lower than the error from the original dataset. So we can't find a skin color bias in red card decisions with a p-value of p=0.05. However, we have doubts if our code is completely correct: surprisingly the error for linear regression is always even lower, when you shuffle the color rating. We do not have an explanation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse linear regression</th>\n",
       "      <th>mse regression tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.005260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.005267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.005269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.005256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.005269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.005263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.005263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.005269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.005266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.005263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.005271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.005270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mse linear regression  mse regression tree\n",
       "0                0.005191             0.005260\n",
       "1                0.005175             0.005267\n",
       "2                0.005184             0.005269\n",
       "3                0.005178             0.005265\n",
       "4                0.005180             0.005268\n",
       "5                0.005178             0.005256\n",
       "6                0.005183             0.005269\n",
       "7                0.005178             0.005264\n",
       "8                0.005175             0.005268\n",
       "9                0.005177             0.005263\n",
       "10               0.005181             0.005265\n",
       "11               0.005180             0.005272\n",
       "12               0.005176             0.005263\n",
       "13               0.005182             0.005269\n",
       "14               0.005182             0.005266\n",
       "15               0.005179             0.005263\n",
       "16               0.005185             0.005271\n",
       "17               0.005177             0.005270\n",
       "18               0.005180             0.005264\n",
       "19               0.005183             0.005268"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_data = pd.read_csv(\"errors.txt\", sep=\",\", index_col=False) \n",
    "err_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 How to Lie With Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already found a choice of features that does not reveal a skin color bias. So we try to find a choice of features that shows such a bias. We choose the \"rating\" column as the only feature. We only apply the Linear Regression model to the data, because our task is to find one example of a choice of features that shows a skin color bias in one of the used models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "5.0 %\n",
      "10.0 %\n",
      "15.0 %\n",
      "20.0 %\n",
      "25.0 %\n",
      "30.0 %\n",
      "35.0 %\n",
      "40.0 %\n",
      "45.0 %\n",
      "50.0 %\n",
      "55.00000000000001 %\n",
      "60.0 %\n",
      "65.0 %\n",
      "70.0 %\n",
      "75.0 %\n",
      "80.0 %\n",
      "85.0 %\n",
      "90.0 %\n",
      "95.0 %\n",
      "\n",
      "error rate, linear regression:\n",
      "0.0052392309104625995\n"
     ]
    }
   ],
   "source": [
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df[[\"rating\"]].to_numpy()\n",
    "df_mean = df_mean[[\"rating\", \"percentageReds\"]]\n",
    "\n",
    "#number of folds\n",
    "L = 20\n",
    "\n",
    "#create  L folds\n",
    "N = len(X)\n",
    "indices = np.random.choice(N, N, replace=False)\n",
    "X_folds = np.array(np.array_split(X[indices], L), dtype=object) \n",
    "Y_folds = np.array(np.array_split(Y[indices], L), dtype=object)\n",
    "\n",
    "#1. Linear Regression\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    regression = LinearRegression()\n",
    "    regression.train(X_train,Y_train)\n",
    "    error.append(compute_error(regression, X_test, Y_test))\n",
    "error = np.mean(error)\n",
    "\n",
    "#print error\n",
    "print(\"\\nerror rate, linear regression:\")\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "5.0 %\n",
      "10.0 %\n",
      "15.0 %\n",
      "20.0 %\n",
      "25.0 %\n",
      "30.0 %\n",
      "35.0 %\n",
      "40.0 %\n",
      "45.0 %\n",
      "50.0 %\n",
      "55.00000000000001 %\n",
      "60.0 %\n",
      "65.0 %\n",
      "70.0 %\n",
      "75.0 %\n",
      "80.0 %\n",
      "85.0 %\n",
      "90.0 %\n",
      "95.0 %\n",
      "\n",
      "error rate, linear regression:\n",
      "0.005248877638861473\n"
     ]
    }
   ],
   "source": [
    "color_rating_index = 0 #index of the color rating in df\n",
    "L = 20 #number of folds\n",
    "\n",
    "#load csv-file where we save the mean squared errors\n",
    "err_data = pd.read_csv(\"errorsLie.txt\", sep=\",\", index_col=False) \n",
    "\n",
    "# load original data set\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df[[\"rating\"]].to_numpy()\n",
    "\n",
    "#1. Linear Regression\n",
    "#shuffle data\n",
    "Y_shuffled = Y\n",
    "X_shuffled = shuffle_data(X, color_rating_index)\n",
    "\n",
    "#create  L folds\n",
    "N = len(X_shuffled)\n",
    "indices = np.random.choice(N, N, replace=False)\n",
    "X_folds = np.array(np.array_split(X_shuffled[indices], L), dtype=object) \n",
    "Y_folds = np.array(np.array_split(Y_shuffled[indices], L), dtype=object)\n",
    "\n",
    "error = []\n",
    "for i in range(L):\n",
    "    print(i/L*100, \"%\")\n",
    "    #create training and test data\n",
    "    X_train = np.concatenate(X_folds[np.arange(L)!=i], axis=0)\n",
    "    Y_train = np.concatenate(Y_folds[np.arange(L)!=i], axis=0)\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    #compute error\n",
    "    regression = LinearRegression()\n",
    "    regression.train(X_train,Y_train)\n",
    "    error.append(compute_error(regression, X_test, Y_test))\n",
    "error = np.mean(error)\n",
    "\n",
    "#print error and save the value\n",
    "print(\"\\nerror rate, linear regression:\")\n",
    "print(error)\n",
    "\n",
    "err_data.loc[len(err_data)] = [error]\n",
    "\n",
    "err_data.to_csv(\"errorsLie.txt\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above 20 times we can find a skin color bias this time: The mean squared error for the shuffled data is always higher than the error for the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Alternative Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise assumes that a correlation between the skin color and the probability to get a red card exists. We did not find such a correlation with our first choice of features. So we assume that the choice of features we used in 2.4 was \"better\". Two causal hypotheses for red cards would then be:\n",
    "1. Heavier players cause more fouls (because the opponent is more likely to fall). This leads to more red cards for players with more weight.\n",
    "2. Players in the position \"Back\" often have to stop an opponent player in the last moment (\"no matter what it costs\"). This leads to more red cards for players in the position \"Back\".\n",
    "If one of these hypotheses is true, we should find a positive correlation between the position \"Back\"/weight and the color rating. \n",
    "Then we would typically expect a positive covariance for these quantities. Additionally, we would expect a positive covarance of the weight/\"Back\" and the probability for a red card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.12806150e+01 3.60300072e-03]\n",
      " [3.60300072e-03 5.17986190e-03]]\n",
      "[[ 5.12806150e+01 -4.98876945e-02]\n",
      " [-4.98876945e-02  8.25710800e-02]] \n",
      "\n",
      "[[0.21255133 0.00071554]\n",
      " [0.00071554 0.00517986]]\n",
      "[[ 0.21255133 -0.00133118]\n",
      " [-0.00133118  0.08257108]]\n"
     ]
    }
   ],
   "source": [
    "#compute covariance matrices\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df[\"weight\"].to_numpy()\n",
    "print(np.cov(X,Y))\n",
    "Y = df[\"rating\"].to_numpy()\n",
    "print(np.cov(X,Y), \"\\n\")\n",
    "\n",
    "Y = df[\"percentageReds\"].to_numpy()\n",
    "X = df[\"Position_Back\"].to_numpy()\n",
    "print(np.cov(X,Y))\n",
    "Y = df[\"rating\"].to_numpy()\n",
    "print(np.cov(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases one of our expectations is not met, which means that our hypotheses are rather not true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
